{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyB/Fw9DMMTgWPWnQMP6Cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JorgeAnsotegui/TFM/blob/main/Entrenamiento/Entrenamiento_Detectron2_Varios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar Colab a Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-3HWc8YqUvkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "uwJ4hP0bUyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "id": "E2X36gMbUz2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.data.datasets import register_coco_instances"
      ],
      "metadata": {
        "id": "GVSYi6zkU3lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPX35KCLUeba"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import optuna\n",
        "import locale\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import torch\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "# Verificación de CUDA\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Número de dispositivos CUDA disponibles:\", torch.cuda.device_count())\n",
        "    print(\"Nombre del dispositivo CUDA actual:\", torch.cuda.get_device_name())\n",
        "\n",
        "# Hiperparámetros ajustados para YOLO\n",
        "HYPERPARAMETERS = {\n",
        "    'epochs': 10000,\n",
        "    'patience': 250,\n",
        "    'batch': [8, 32],\n",
        "    'workers': [8, 16],\n",
        "    'imgsz': [320, 1024],\n",
        "    'save_period': -1,\n",
        "    'num_trials': 5,\n",
        "    'lr0': [1e-5, 1e-2],\n",
        "    'momentum': [0.7, 0.99],\n",
        "    'weight_decay': [1e-5, 1e-2],\n",
        "    'warmup_epochs': [100, 1000],\n",
        "    'warmup_momentum': [0.0, 0.95],\n",
        "    'warmup_bias_lr': [1e-5, 1e-2],\n",
        "    'optimizer': ['RMSProp', 'Adam', 'SGD'],\n",
        "    'testing_threshold': [0.4, 0.6]\n",
        "}\n",
        "\n",
        "# Combinaciones de datasets y pesos preentrenados\n",
        "combinaciones = [\n",
        "    {\"ruta_resultados\": \"models/Final/YoloV8_m_20_fix\", \"archivo_yaml\": \"/home/hpc22/computer_vision_colon/TFM/Entrenamiento/datasets/Polipos265_Detectron2FIX_YoloV8/data.yaml\", \"pre_weights\": \"yolov8m-seg.yaml\"},\n",
        "    {\"ruta_resultados\": \"models/Final/YoloV8_m_20_pre\", \"archivo_yaml\": \"/home/hpc22/computer_vision_colon/TFM/Entrenamiento/datasets/DataAugmentation/Only_Preprocessing/data.yaml\", \"pre_weights\": \"yolov8m-seg.yaml\"},\n",
        "    {\"ruta_resultados\": \"models/Final/YoloV8_m_20_all\", \"archivo_yaml\": \"/home/hpc22/computer_vision_colon/TFM/Entrenamiento/datasets/DataAugmentation/All_Data_Augmentation/data.yaml\", \"pre_weights\": \"yolov8m-seg.yaml\"}\n",
        "]\n",
        "\n",
        "# Función objetivo para Optuna\n",
        "def objective(trial, ruta_resultados, archivo_yaml, pre_weights):\n",
        "    lr0 = trial.suggest_float('lr0', HYPERPARAMETERS['lr0'][0], HYPERPARAMETERS['lr0'][1], log=True)\n",
        "    momentum = trial.suggest_float('momentum', HYPERPARAMETERS['momentum'][0], HYPERPARAMETERS['momentum'][1])\n",
        "    weight_decay = trial.suggest_float('weight_decay', HYPERPARAMETERS['weight_decay'][0], HYPERPARAMETERS['weight_decay'][1], log=True)\n",
        "    warmup_epochs = trial.suggest_int('warmup_epochs', HYPERPARAMETERS['warmup_epochs'][0], HYPERPARAMETERS['warmup_epochs'][1])\n",
        "    warmup_momentum = trial.suggest_float('warmup_momentum', HYPERPARAMETERS['warmup_momentum'][0], HYPERPARAMETERS['warmup_momentum'][1])\n",
        "    warmup_bias_lr = trial.suggest_float('warmup_bias_lr', HYPERPARAMETERS['warmup_bias_lr'][0], HYPERPARAMETERS['warmup_bias_lr'][1], log=True)\n",
        "    optimizer = trial.suggest_categorical('optimizer', HYPERPARAMETERS['optimizer'])\n",
        "    testing_threshold = trial.suggest_float('testing_threshold', HYPERPARAMETERS['testing_threshold'][0], HYPERPARAMETERS['testing_threshold'][1])\n",
        "\n",
        "    batch_size = trial.suggest_int('batch', HYPERPARAMETERS['batch'][0], HYPERPARAMETERS['batch'][1], step=8)\n",
        "    workers = trial.suggest_int('workers', HYPERPARAMETERS['workers'][0], HYPERPARAMETERS['workers'][1])\n",
        "    imgsz = trial.suggest_int('imgsz', HYPERPARAMETERS['imgsz'][0], HYPERPARAMETERS['imgsz'][1], step=64)\n",
        "\n",
        "    # Crear una instancia del modelo\n",
        "    model = YOLO(pre_weights)\n",
        "\n",
        "    # Configurar los parámetros de entrenamiento\n",
        "    training_params = {\n",
        "        'data': archivo_yaml,\n",
        "        'project': ruta_resultados,\n",
        "        'name': f\"trial_{trial.number}\",\n",
        "        'epochs': HYPERPARAMETERS['epochs'],\n",
        "        'patience': HYPERPARAMETERS['patience'],\n",
        "        'batch': batch_size,\n",
        "        'imgsz': imgsz,\n",
        "        'lr0': lr0,\n",
        "        'momentum': momentum,\n",
        "        'weight_decay': weight_decay,\n",
        "        'warmup_epochs': warmup_epochs,\n",
        "        'warmup_momentum': warmup_momentum,\n",
        "        'warmup_bias_lr': warmup_bias_lr,\n",
        "        'optimizer': optimizer,\n",
        "        'workers': workers,\n",
        "        'save_period': HYPERPARAMETERS['save_period']\n",
        "    }\n",
        "\n",
        "    # Entrenar el modelo con los hiperparámetros sugeridos\n",
        "    results = model.train(**training_params)\n",
        "\n",
        "    # Definir la ruta para la carpeta del trial\n",
        "    trial_dir = os.path.join(ruta_resultados, f\"trial_{trial.number}\")\n",
        "    if not os.path.exists(trial_dir):\n",
        "        os.makedirs(trial_dir)\n",
        "\n",
        "    # Definir la ruta para la carpeta del trial y el archivo all_metrics.json\n",
        "    model_path = os.path.join(trial_dir, \"weights/best.pt\")\n",
        "\n",
        "    # Cargamos el modelo que se acaba de entrenar\n",
        "    model_segmentation = YOLO(model_path)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de test con el umbral de confianza especificado\n",
        "    val_params = {\n",
        "        'data': archivo_yaml,\n",
        "        'conf': testing_threshold,\n",
        "        'save_json': True,\n",
        "        'split': \"test\",\n",
        "        'project': trial_dir,\n",
        "        'name': \"Evaluacion\"\n",
        "    }\n",
        "    print(\"Antes de validar\")\n",
        "    test_results = model_segmentation.val(**val_params)\n",
        "    print(\"Despues de validar\")\n",
        "\n",
        "    # Obtener las métricas del modelo\n",
        "    metrics = test_results.results_dict\n",
        "\n",
        "    # Crear un DataFrame con las métricas\n",
        "    df = pd.DataFrame([metrics])\n",
        "\n",
        "    # Extraer y convertir las métricas\n",
        "    precision = df[\"metrics/precision(M)\"].iloc[0]\n",
        "    recall = df[\"metrics/recall(M)\"].iloc[0]\n",
        "    mAP50 = df[\"metrics/mAP50(M)\"].iloc[0]\n",
        "\n",
        "    # Imprimir las métricas para verificar\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"mAP: {mAP50}\")\n",
        "\n",
        "    # Crear un diccionario con las métricas relevantes\n",
        "    metrics_row = {\n",
        "        \"Trial\": trial.number,\n",
        "        \"Dataset\": archivo_yaml,\n",
        "        \"Feedback\": \"mAP50\",\n",
        "        \"Pre-Trained_Weights\": pre_weights,\n",
        "        \"metrics\": {\n",
        "            'precision': metrics.get('metrics/precision(M)', 0),\n",
        "            'recall': metrics.get('metrics/recall(M)', 0),\n",
        "            'mAP50': metrics.get('metrics/mAP50(M)', 0),\n",
        "            'mAP50-95': metrics.get('metrics/mAP50-95(M)', 0),\n",
        "            'fitness': metrics.get('fitness', 0)\n",
        "        },\n",
        "        \"hyperparameters\": {\n",
        "            'lr0': lr0,\n",
        "            'epochs': HYPERPARAMETERS['epochs'],\n",
        "            'batch_size': batch_size,\n",
        "            'imgsz': imgsz,\n",
        "            'momentum': momentum,\n",
        "            'weight_decay': weight_decay,\n",
        "            'warmup_epochs': warmup_epochs,\n",
        "            'warmup_momentum': warmup_momentum,\n",
        "            'warmup_bias_lr': warmup_bias_lr,\n",
        "            'optimizer': optimizer,\n",
        "            'workers': workers,\n",
        "            'testing_threshold': testing_threshold\n",
        "        }\n",
        "    }\n",
        "    metrics_path = os.path.join(trial_dir, \"metrics.json\")\n",
        "    with open(metrics_path, 'w') as outfile:\n",
        "        json.dump(metrics_row, outfile)\n",
        "\n",
        "    return mAP50\n",
        "\n",
        "# Bucle para ejecutar el entrenamiento y la optimización para cada combinación de rutas y pesos\n",
        "for combinacion in combinaciones:\n",
        "    ruta_resultados = combinacion[\"ruta_resultados\"]\n",
        "    archivo_yaml = combinacion[\"archivo_yaml\"]\n",
        "    pre_weights = combinacion[\"pre_weights\"]\n",
        "\n",
        "    # Ejecutar la optimización con Optuna\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(lambda trial: objective(trial, ruta_resultados, archivo_yaml, pre_weights), n_trials=HYPERPARAMETERS['num_trials'])"
      ]
    }
  ]
}