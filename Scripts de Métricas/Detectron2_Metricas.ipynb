{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de los Hyperparametros Optimizados para modelo de detección de Detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importamos las librerias y modulos necesarios:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detectron2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_zoo\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.structures import Boxes\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import optuna\n",
    "import locale\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegurar que hay una gpu disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Número de dispositivos CUDA disponibles:\", torch.cuda.device_count())\n",
    "    print(\"Nombre del dispositivo CUDA actual:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rutas del directorio donde se han guarrdad los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_resultados = \"models/Final/Detectron2_s_20_all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juntamos todas las metricas en un solo archivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de carpetas que siguen el patrón \"trial_\"\n",
    "trial_folders = [folder for folder in os.listdir(ruta_resultados) if folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit()]\n",
    "n_trials = len(trial_folders)\n",
    "print(n_trials)\n",
    "\n",
    "# Crear una lista para almacenar todas las métricas\n",
    "all_metrics = []\n",
    "\n",
    "# Recorrer todas las carpetas de trials y leer los archivos JSON de métricas\n",
    "for trial_num in range(n_trials):\n",
    "    trial_dir = os.path.join(ruta_resultados, f\"trial_{trial_num}\")\n",
    "#     result_path = os.path.join(trial_dir, f\"trial_{trial_num}_result.json\")\n",
    "    result_path = os.path.join(trial_dir, f\"metrics.json\")\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, 'r') as f:\n",
    "            trial_metrics = json.load(f)\n",
    "            trial_metrics[\"trial_number\"] = trial_num  # Añadir el número del trial a las métricas\n",
    "            all_metrics.append(trial_metrics)\n",
    "\n",
    "# Guardar todas las métricas en un archivo JSON\n",
    "all_metrics_path = os.path.join(ruta_resultados, \"all_metrics.json\")\n",
    "with open(all_metrics_path, 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print(f\"Métricas de todos los trials guardadas en {all_metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se verifica con que dataset fue entrenado el modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer el archivo all_metrics.json\n",
    "with open(all_metrics_path, 'r') as f:\n",
    "    all_metrics = json.load(f)\n",
    "\n",
    "# Extraer el valor del campo \"Dataset\"\n",
    "if all_metrics:\n",
    "    data_yaml_path = all_metrics[1][\"Dataset\"]\n",
    "else:\n",
    "    data_yaml_path = None\n",
    "\n",
    "# Extraer el valor del campo \"Dataset\"\n",
    "if all_metrics:\n",
    "    pretrained_weights = all_metrics[1][\"Pre-Trained_Weights\"]\n",
    "else:\n",
    "    pretrained_weights = None\n",
    "\n",
    "print(f\"El archivo YAML utilizado es: {data_yaml_path}\")\n",
    "print(f\"Los pesos preentrenados usados son: {pretrained_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_directorio(directorio):\n",
    "    \"\"\"Crea un directorio si no existe.\"\"\"\n",
    "    print(directorio)\n",
    "    if not os.path.exists(directorio):\n",
    "        os.makedirs(directorio)\n",
    "        print(f\"Directorio '{directorio}' creado.\")\n",
    "    else:\n",
    "        print(f\"El directorio '{directorio}' ya existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_trial(trial_path, data_yaml_path):\n",
    "    print(\"Iniciando procesar_trial\")\n",
    "    # Definir la ruta para la carpeta del trial y el archivo all_metrics.json\n",
    "    model_path = os.path.join(trial_path, \"model_final.pth\")  # Archivo .pth para Detectron2\n",
    "    print(\"\\n\\nCargado modelo: \", model_path)\n",
    "    \n",
    "    # Crear carpeta para coordenadas\n",
    "    output_folder = os.path.join(trial_path, \"coordenadas_test\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Carpeta de salida creada: {output_folder}\")\n",
    "    \n",
    "    # Modificar la ruta, subir dos directorios y añadir /test/annotations.json\n",
    "    data_test_yaml_path = os.path.join(Path(data_yaml_path).parents[1], 'test', 'annotations.json')\n",
    "    print(f\"Ruta de anotaciones de prueba: {data_test_yaml_path}\")\n",
    "    # Obtener el directorio de test\n",
    "    test_folder = Path(data_test_yaml_path).parent\n",
    "    \n",
    "    image_test_folder = os.path.join(test_folder, 'images')\n",
    "    print(f\"Carpeta de imágenes de prueba: {image_test_folder}\")\n",
    "    \n",
    "    print(\"Configurando el modelo Detectron2\")\n",
    "    # Configuración del modelo Detectron2\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(pretrained_weights))\n",
    "    cfg.MODEL.WEIGHTS = model_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Umbral de confianza\n",
    "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    print(f\"Dispositivo utilizado: {cfg.MODEL.DEVICE}\")\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Lista para almacenar todas las predicciones\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(\"Iniciando procesamiento de imágenes\")\n",
    "    # Procesar imágenes\n",
    "    for img_path in Path(image_test_folder).glob('*'):\n",
    "        print(f\"Procesando imagen: {img_path}\")\n",
    "        # Leer la imagen y hacer predicciones\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            print(f\"Error: No se pudo leer la imagen {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        img_height, img_width = img.shape[:2]\n",
    "        print(f\"Tamaño de la imagen: {img_width}x{img_height}\")\n",
    "        \n",
    "        print(\"Realizando predicción\")\n",
    "        outputs = predictor(img)\n",
    "        \n",
    "        # Extraer las predicciones: cajas, clases, puntuaciones y máscaras\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        pred_boxes = instances.pred_boxes.tensor.numpy() if instances.has(\"pred_boxes\") else None\n",
    "        pred_classes = instances.pred_classes.numpy() if instances.has(\"pred_classes\") else None\n",
    "        scores = instances.scores.numpy() if instances.has(\"scores\") else None\n",
    "        pred_masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
    "        \n",
    "        print(f\"Número de predicciones: {len(pred_boxes) if pred_boxes is not None else 0}\")\n",
    "        \n",
    "        # Crear predicciones en formato Detectron2\n",
    "        for i in range(len(pred_boxes)):\n",
    "            prediction = {\n",
    "                \"image_id\": img_path.stem,\n",
    "                \"category_id\": int(pred_classes[i]),\n",
    "                \"bbox\": pred_boxes[i].tolist(),\n",
    "                \"score\": float(scores[i])\n",
    "            }\n",
    "            if pred_masks is not None:\n",
    "                # Convertir la máscara a coordenadas\n",
    "                mask_coords = mask_to_coordinates(pred_masks[i], img_width, img_height)\n",
    "                print(f\"Formato de coordenadas de la máscara {i} para {img_path.stem}:\")\n",
    "                print(mask_coords)\n",
    "                \n",
    "                # Guardar las coordenadas en el JSON\n",
    "                prediction[\"segmentation\"] = mask_coords\n",
    "            \n",
    "            all_predictions.append(prediction)\n",
    "        \n",
    "        print(f\"Predicciones procesadas para: {img_path}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"Total de predicciones: {len(all_predictions)}\")\n",
    "    \n",
    "    # Guardar todas las predicciones en un único archivo JSON\n",
    "    output_file = os.path.join(output_folder, \"predictions.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_predictions, f, indent=2)\n",
    "    \n",
    "    print(f\"Todas las predicciones guardadas en: {output_file}\")\n",
    "\n",
    "def mask_to_coordinates(mask, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert a binary mask to a list of (coor_X, coor_Y) coordinates.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    coordinates = []\n",
    "    for contour in contours:\n",
    "        for point in contour:\n",
    "            x, y = point[0]\n",
    "            # Asegurarse de que las coordenadas estén dentro del rango de la imagen\n",
    "            x = min(max(x, 0), img_width - 1)\n",
    "            y = min(max(y, 0), img_height - 1)\n",
    "            coordinates.extend([float(x), float(y)])\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_coordenadas_test(trial_path):\n",
    "    \"\"\"Eliminar la carpeta coordenadas_test si existe dentro de trial_path.\"\"\"\n",
    "    coordenadas_test_path = os.path.join(trial_path, \"coordenadas_test\")\n",
    "    if os.path.exists(coordenadas_test_path) and os.path.isdir(coordenadas_test_path):\n",
    "        shutil.rmtree(coordenadas_test_path)\n",
    "        print(f\"Carpeta 'coordenadas_test' eliminada en: {trial_path}\")\n",
    "    else:\n",
    "        print(f\"No se encontró la carpeta 'coordenadas_test' en: {trial_path}\")\n",
    "        \n",
    "# Iterar sobre cada carpeta trial y borrar coordenadas_test\n",
    "for folder in os.listdir(ruta_resultados):\n",
    "    trial_path = os.path.join(ruta_resultados, folder)\n",
    "    if os.path.isdir(trial_path) and folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit():\n",
    "        borrar_coordenadas_test(trial_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada carpeta trial y procesar\n",
    "for folder in os.listdir(ruta_resultados):\n",
    "    trial_path = os.path.join(ruta_resultados, folder)\n",
    "    if os.path.isdir(trial_path) and folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit():\n",
    "        procesar_trial(trial_path,data_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplanar_segmentacion(segmentacion):\n",
    "    \"\"\"\n",
    "    Aplana la lista de segmentación de coordenadas, eliminando corchetes anidados.\n",
    "    \"\"\"\n",
    "    return [coordenadas for grupo in segmentacion for coordenadas in grupo]\n",
    "\n",
    "def evaluar_trial(trial_path, data_yaml_path):\n",
    "    # Construir las rutas de los archivos JSON\n",
    "    ground_truths_path = os.path.join(Path(data_yaml_path).parents[1], 'test', 'annotations.json')\n",
    "    predictions_path = os.path.join(trial_path, \"coordenadas_test\", \"predictions.json\")\n",
    "    \n",
    "    print(f\"Ruta de ground_truths: {ground_truths_path}\")\n",
    "    print(f\"Ruta de predictions: {predictions_path}\")\n",
    "    \n",
    "    # Cargar los archivos JSON\n",
    "    try:\n",
    "        with open(ground_truths_path) as f:\n",
    "            ground_truths = json.load(f)\n",
    "        print(\"Cargado ground_truths.json exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar ground_truths.json: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(predictions_path) as f:\n",
    "            predictions = json.load(f)\n",
    "        print(\"Cargado predictions.json exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar predictions.json: {e}\")\n",
    "        return\n",
    "\n",
    "    # Crear un diccionario para mapear file_name (sin extensión) a image_id en ground_truths\n",
    "    file_name_to_image_id = {os.path.splitext(img['file_name'])[0]: img['id'] for img in ground_truths.get('images', [])}\n",
    "    print(f\"Diccionario file_name_to_image_id: {file_name_to_image_id}\")\n",
    "\n",
    "    # Crear un diccionario para mapear image_id a sus anotaciones en ground_truths\n",
    "    annotations_by_image = {}\n",
    "    image_info = {}  # Nuevo diccionario para guardar información de altura y anchura\n",
    "\n",
    "    for img in ground_truths.get('images', []):\n",
    "        image_id = img['id']\n",
    "        image_info[image_id] = {'file_name': img['file_name'], 'height': img['height'], 'width': img['width']}\n",
    "    \n",
    "    for ann in ground_truths.get('annotations', []):\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in annotations_by_image:\n",
    "            annotations_by_image[image_id] = {'segmentation_GT': []}\n",
    "        annotations_by_image[image_id]['segmentation_GT'].append(ann['segmentation'])\n",
    "    \n",
    "#     print(f\"Diccionario annotations_by_image: {annotations_by_image}\")\n",
    "#     print(f\"Diccionario image_info: {image_info}\")\n",
    "\n",
    "    # Crear un diccionario para mapear image_id (file_name en predictions) a sus predicciones en predictions\n",
    "    predictions_by_image = {}\n",
    "    for pred in predictions:\n",
    "        file_name = pred['image_id']\n",
    "        if file_name in file_name_to_image_id:\n",
    "            image_id = file_name_to_image_id[file_name]\n",
    "            if image_id not in predictions_by_image:\n",
    "                predictions_by_image[image_id] = {'category_id': pred['category_id'], 'segmentation_Pred': []}\n",
    "            predictions_by_image[image_id]['segmentation_Pred'].append(pred['segmentation'])\n",
    "        else:\n",
    "            print(f\"Advertencia: El file_name '{file_name}' de predictions no se encuentra en ground_truths.\")\n",
    "\n",
    "#     print(f\"Diccionario predictions_by_image: {predictions_by_image}\")\n",
    "\n",
    "    # Emparejar las coordenadas de segmentación\n",
    "    results = []\n",
    "    for image_id, anns in annotations_by_image.items():\n",
    "        if image_id in predictions_by_image:\n",
    "            file_name = image_info[image_id]['file_name']\n",
    "            result = {\n",
    "                'file_name': file_name,\n",
    "                'category_id': predictions_by_image[image_id]['category_id'],\n",
    "                'height': image_info[image_id]['height'],\n",
    "                'width': image_info[image_id]['width'],\n",
    "                'segmentation_GT': aplanar_segmentacion(anns['segmentation_GT']),\n",
    "                'segmentation_Pred': aplanar_segmentacion(predictions_by_image[image_id]['segmentation_Pred'])\n",
    "                \n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(f\"Resultados emparejados: {results}\")\n",
    "\n",
    "    # Guardar el resultado en un nuevo archivo JSON\n",
    "    output_path = os.path.join(trial_path, \"coordenadas_test/matched_segments.json\")\n",
    "    try:\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"El emparejamiento de coordenadas de segmentación se ha guardado en '{output_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar matched_segments.json: {e}\")\n",
    "\n",
    "# Iterar sobre cada carpeta trial y procesar\n",
    "for folder in os.listdir(ruta_resultados):\n",
    "    trial_path = os.path.join(ruta_resultados, folder)\n",
    "    if os.path.isdir(trial_path) and folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit():\n",
    "        evaluar_trial(trial_path, data_yaml_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para dibujar las máscaras en una imagen de fondo negro\n",
    "def draw_masks_on_black_background(gt_mask, pred_mask, img_width, img_height):\n",
    "    # Crear una imagen negra\n",
    "    black_background = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Crear máscaras de colores\n",
    "    gt_colored = np.stack([gt_mask * 255, np.zeros_like(gt_mask), np.zeros_like(gt_mask)], axis=-1)  # Azul\n",
    "    pred_colored = np.stack([np.zeros_like(pred_mask), np.zeros_like(pred_mask), pred_mask * 255], axis=-1)  # Rojo\n",
    "    \n",
    "    # Superponer las máscaras en la imagen negra con transparencia\n",
    "    combined_image = cv2.addWeighted(black_background, 1, gt_colored, 0.5, 0)\n",
    "    combined_image = cv2.addWeighted(combined_image, 1, pred_colored, 0.5, 0)\n",
    "    \n",
    "    return combined_image\n",
    "\n",
    "# Función para dibujar contornos de las máscaras en la imagen original\n",
    "def draw_contours_on_image(image, gt_mask, pred_mask):\n",
    "    contours_gt, _ = cv2.findContours(gt_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_pred, _ = cv2.findContours(pred_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    image_contours = image.copy()\n",
    "    cv2.drawContours(image_contours, contours_gt, -1, (255, 0, 0), 2)  # Azul para GT\n",
    "    cv2.drawContours(image_contours, contours_pred, -1, (0, 0, 255), 2)  # Rojo para Pred\n",
    "    \n",
    "    return image_contours\n",
    "\n",
    "# Función para crear máscara binaria a partir de las coordenadas\n",
    "def create_binary_mask(coords_list, img_width, img_height):\n",
    "    mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    if isinstance(coords_list[0], (int, float)):\n",
    "        coords_list = [coords_list]  # Envolver en una lista para uniformidad\n",
    "    \n",
    "    for coords in coords_list:\n",
    "        coords = np.array(coords)\n",
    "        if coords.ndim == 1:\n",
    "            coords = coords.reshape(-1, 2)\n",
    "        coords = coords.astype(np.int32)\n",
    "        \n",
    "        if coords.shape[0] > 0:\n",
    "            cv2.fillPoly(mask, [coords], 1)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Modificar la función evaluar_trial para usar la nueva función\n",
    "def evaluar_trial(trial_path, dataset_path):\n",
    "    # Extraer número del trial\n",
    "    trial_number = extract_trial_number(trial_path)\n",
    "    if trial_number is None:\n",
    "        raise ValueError(f\"No se pudo extraer el número del trial de la ruta: {trial_path}\")\n",
    "\n",
    "    # Definir la ruta del archivo matched_segments.json\n",
    "    json_file = os.path.join(trial_path, \"coordenadas_test\", \"matched_segments.json\")\n",
    "    \n",
    "    # Cargar el archivo JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Seleccionar 3 archivos aleatorios\n",
    "    random_files = random.sample(data, min(3, len(data)))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Procesar cada imagen en el archivo JSON\n",
    "    for item in data:\n",
    "        file_name = item[\"file_name\"]\n",
    "        if not file_name.endswith(\".jpg\"):\n",
    "            file_name += \".jpg\"  # Añadir la extensión si no está presente\n",
    "\n",
    "        img_width, img_height = item[\"width\"], item[\"height\"]\n",
    "        segmentation_gt = item.get('segmentation_GT', [])\n",
    "        segmentation_pred = item.get('segmentation_Pred', [])\n",
    "\n",
    "        # Cargar la imagen original\n",
    "        image_path = os.path.join(dataset_path, \"test/images\", file_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: No se pudo cargar la imagen {image_path}. Verifica la ruta y el archivo.\")\n",
    "            continue  # Saltar esta imagen si no se carga correctamente\n",
    "\n",
    "        # Crear máscaras binarias a partir de las coordenadas\n",
    "        gt_mask = create_binary_mask(segmentation_gt, img_width, img_height)\n",
    "        pred_mask = create_binary_mask(segmentation_pred, img_width, img_height)\n",
    "        \n",
    "        # Binarizar las máscaras\n",
    "        binary_true_mask = (gt_mask > 0).astype(int)\n",
    "        binary_predicted_mask = (pred_mask > 0).astype(int)\n",
    "        \n",
    "        # Calcular TP, TN, FP, FN\n",
    "        TP = np.sum((binary_predicted_mask == 1) & (binary_true_mask == 1))\n",
    "        FP = np.sum((binary_predicted_mask == 1) & (binary_true_mask == 0))\n",
    "        TN = np.sum((binary_predicted_mask == 0) & (binary_true_mask == 0))\n",
    "        FN = np.sum((binary_predicted_mask == 0) & (binary_true_mask == 1))\n",
    "        \n",
    "        results.append([file_name, TP, TN, FP, FN])\n",
    "\n",
    "        # Generar figuras para las 3 imágenes seleccionadas\n",
    "        if item in random_files:\n",
    "            # Dibujar máscaras en fondo negro\n",
    "            combined_image = draw_masks_on_black_background(gt_mask, pred_mask, img_width, img_height)\n",
    "            \n",
    "            # Guardar la imagen combinada\n",
    "            figuras_folder = os.path.join(trial_path, 'Figuras')\n",
    "            if not os.path.exists(figuras_folder):\n",
    "                os.makedirs(figuras_folder)\n",
    "\n",
    "            combined_image_file = os.path.join(figuras_folder, f\"{file_name}_mask_eval.png\")\n",
    "            cv2.imwrite(combined_image_file, combined_image)\n",
    "\n",
    "            # Dibujar contornos de las máscaras en la imagen original\n",
    "            contour_image = draw_contours_on_image(image, gt_mask, pred_mask)\n",
    "            visual_image_file = os.path.join(figuras_folder, f\"{file_name}_mask_visual.png\")\n",
    "            cv2.imwrite(visual_image_file, contour_image)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_df = pd.DataFrame(results, columns=['Filename', 'TP', 'TN', 'FP', 'FN'])\n",
    "    mean_TP = results_df['TP'].mean()\n",
    "    mean_TN = results_df['TN'].mean()\n",
    "    mean_FP = results_df['FP'].mean()\n",
    "    mean_FN = results_df['FN'].mean()\n",
    "    \n",
    "    precision_avg = mean_TP / (mean_TP + mean_FP) if (mean_TP + mean_FP) > 0 else 0\n",
    "    recall_avg = mean_TP / (mean_TP + mean_FN) if (mean_TP + mean_FN) > 0 else 0\n",
    "    f1_score_avg = (2 * precision_avg * recall_avg) / (precision_avg + recall_avg) if (precision_avg + recall_avg) > 0 else 0\n",
    "    iou_avg = mean_TP / (mean_TP + mean_FP + mean_FN) if (mean_TP + mean_FP + mean_FN) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'mean_TP': mean_TP,\n",
    "        'mean_TN': mean_TN,\n",
    "        'mean_FP': mean_FP,\n",
    "        'mean_FN': mean_FN,\n",
    "        'precision_avg': precision_avg,\n",
    "        'recall_avg': recall_avg,\n",
    "        'f1_score_avg': f1_score_avg,\n",
    "        'iou_avg': iou_avg\n",
    "    }\n",
    "    \n",
    "    metrics_file = os.path.join(trial_path, f\"trial_{trial_number}_metricas.json\")\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Resultados guardados en {metrics_file}\")\n",
    "\n",
    "# Función para extraer el número de trial\n",
    "def extract_trial_number(trial_path):\n",
    "    import re\n",
    "    match = re.search(r\"trial_(\\d+)\", trial_path)\n",
    "    if match:\n",
    "        return int(match.group(1))  # Devuelve el número como entero\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_Figuras(trial_path):\n",
    "    \"\"\"Eliminar la carpeta Figuras si existe dentro de trial_path.\"\"\"\n",
    "    Figuras_path = os.path.join(trial_path, \"Figuras\")\n",
    "    if os.path.exists(Figuras_path) and os.path.isdir(Figuras_path):\n",
    "        shutil.rmtree(Figuras_path)\n",
    "        print(f\"Carpeta 'Figuras' eliminada en: {trial_path}\")\n",
    "    else:\n",
    "        print(f\"No se encontró la carpeta 'Figuras' en: {trial_path}\")\n",
    "        \n",
    "# Iterar sobre cada carpeta trial y borrar Figuras\n",
    "for folder in os.listdir(ruta_resultados):\n",
    "    trial_path = os.path.join(ruta_resultados, folder)\n",
    "    if os.path.isdir(trial_path) and folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit():\n",
    "        borrar_Figuras(trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(Path(data_yaml_path).parent).parent\n",
    "\n",
    "# Iterar sobre cada carpeta trial y procesar\n",
    "for folder in os.listdir(ruta_resultados):\n",
    "    trial_path = os.path.join(ruta_resultados, folder)\n",
    "    if os.path.isdir(trial_path) and folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit():\n",
    "        evaluar_trial(trial_path,dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juntamos todos los json de las metricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de carpetas que siguen el patrón \"trial_\"\n",
    "trial_folders = [folder for folder in os.listdir(ruta_resultados) if folder.startswith(\"trial_\") and folder.split(\"_\")[1].isdigit()]\n",
    "n_trials = len(trial_folders)\n",
    "\n",
    "# Crear una lista para almacenar todas las métricas\n",
    "all_metrics = []\n",
    "\n",
    "# Recorrer todas las carpetas de trials y leer los archivos JSON de métricas\n",
    "for trial_num in range(n_trials):\n",
    "    trial_dir = os.path.join(ruta_resultados, f\"trial_{trial_num}\")\n",
    "    metrics_path = os.path.join(trial_dir, f\"trial_{trial_num}_metricas.json\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            trial_metrics = json.load(f)\n",
    "            trial_metrics[\"trial_number\"] = trial_num  # Añadir el número del trial a las métricas\n",
    "            all_metrics.append(trial_metrics)\n",
    "\n",
    "# Guardar todas las métricas en un archivo JSON\n",
    "evaluacion_path = os.path.join(ruta_resultados, \"evaluacion.json\")\n",
    "with open(evaluacion_path, 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print(f\"Métricas de todos los trials guardadas en {evaluacion_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un criterio por el que obtenemos el mejor modelo de estos trials, segun la media de sus métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo JSON\n",
    "with open(evaluacion_path, 'r') as f:\n",
    "    metrics_data = json.load(f)\n",
    "\n",
    "# Definir pesos para cada métrica (puedes ajustar los pesos según tu criterio)\n",
    "weights = {\n",
    "    \"precision_avg\": 0.25,\n",
    "    \"recall_avg\": 0.25,\n",
    "    \"f1_score_avg\": 0.25,\n",
    "    \"iou_avg\": 0.25\n",
    "}\n",
    "\n",
    "# Crear una lista con los trials y sus medias ponderadas\n",
    "trials_with_scores = []\n",
    "\n",
    "for trial in metrics_data:\n",
    "    precision = trial.get(\"precision_avg\", 0)\n",
    "    recall = trial.get(\"recall_avg\", 0)\n",
    "    f1_score = trial.get(\"f1_score_avg\", 0)\n",
    "    iou = trial.get(\"iou_avg\", 0)\n",
    "    \n",
    "    # Calcular la media ponderada\n",
    "    weighted_score = (\n",
    "        weights[\"precision_avg\"] * precision +\n",
    "        weights[\"recall_avg\"] * recall +\n",
    "        weights[\"f1_score_avg\"] * f1_score +\n",
    "        weights[\"iou_avg\"] * iou\n",
    "    )\n",
    "    \n",
    "    # Añadir a la lista\n",
    "    trials_with_scores.append({\n",
    "        \"trial_number\": trial[\"trial_number\"],\n",
    "        \"weighted_score\": weighted_score\n",
    "    })\n",
    "\n",
    "# Ordenar los trials por la media ponderada de mayor a menor\n",
    "sorted_trials = sorted(trials_with_scores, key=lambda x: x[\"weighted_score\"], reverse=True)\n",
    "\n",
    "# Mostrar la lista ordenada\n",
    "for trial in sorted_trials:\n",
    "    print(f\"Trial {trial['trial_number']}: Weighted Score = {trial['weighted_score']}\")\n",
    "\n",
    "# Si quieres guardar el resultado en un archivo JSON\n",
    "sorted_trials_path = os.path.join(os.path.dirname(evaluacion_path), \"sorted_trials.json\")\n",
    "with open(sorted_trials_path, 'w') as f:\n",
    "    json.dump(sorted_trials, f, indent=2)\n",
    "\n",
    "print(f\"Lista de trials ordenados guardada en {sorted_trials_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un criterio por el que obtenemos el mejor modelo de estos trials, segun el teimpo de inferencia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Inicializar variables\n",
    "    resultados_inferencia = []\n",
    "\n",
    "    # Obtener la ruta de las imágenes originales\n",
    "    dataset_path = os.path.dirname(data_yaml_path)\n",
    "    dataset_test_labels = os.path.join(dataset_path, 'test/labels')\n",
    "    ground_truths = dataset_test_labels\n",
    "    dataset_test_path = os.path.dirname(ground_truths)\n",
    "    ruta_imagenes_test = os.path.join(dataset_test_path, 'images')\n",
    "\n",
    "    # Obtener todas las carpetas de trials\n",
    "    for trial_folder in os.listdir(ruta_resultados):\n",
    "        trial_path = os.path.join(ruta_resultados, trial_folder)\n",
    "\n",
    "        # Verificar si la carpeta es un trial\n",
    "        if os.path.isdir(trial_path) and trial_folder.startswith(\"trial_\"):\n",
    "            trial_number = int(trial_folder.split(\"_\")[1])\n",
    "\n",
    "            # Cargar el modelo Detectron2 del trial\n",
    "            config_file = os.path.join(trial_path, \"config.yaml\")  # Ruta al archivo de configuración\n",
    "            model_path = os.path.join(trial_path, \"model_final.pth\")  # Ruta al archivo de pesos\n",
    "\n",
    "            # Configuración del modelo Detectron2\n",
    "            cfg = get_cfg()\n",
    "            cfg.merge_from_file(model_zoo.get_config_file(pretrained_weights))\n",
    "            cfg.MODEL.WEIGHTS = model_path\n",
    "            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Umbral de confianza\n",
    "            cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "            print(f\"Dispositivo utilizado: {cfg.MODEL.DEVICE}\")\n",
    "            predictor = DefaultPredictor(cfg)\n",
    "\n",
    "            predictor = DefaultPredictor(cfg)\n",
    "            print(f\"\\nModelo cargado para el trial {trial_number}\")\n",
    "\n",
    "            # Obtener las imágenes de test\n",
    "            test_folder = os.path.join(Path(data_yaml_path).parents[1], 'test', 'images')\n",
    "            print(f\"test_folder: \", test_folder)\n",
    "            image_paths = list(Path(test_folder).glob('*'))\n",
    "\n",
    "            # Inicializar el contador de tiempo\n",
    "            total_inference_time = 0\n",
    "            num_images = len(image_paths)\n",
    "\n",
    "            # Realizar la inferencia sobre las imágenes y medir el tiempo\n",
    "            for img_path in image_paths:\n",
    "                print(f\"Procesando imagen: {img_path}\")\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Leer la imagen\n",
    "                image = cv2.imread(str(img_path))\n",
    "                if image is None:\n",
    "                    print(f\"Error: No se pudo cargar la imagen {img_path}.\")\n",
    "                    continue\n",
    "\n",
    "                # Ejecutar inferencia\n",
    "                outputs = predictor(image)\n",
    "\n",
    "                # Calcular tiempo de inferencia\n",
    "                inference_time = time.time() - start_time\n",
    "                total_inference_time += inference_time\n",
    "\n",
    "                # Mostrar el tiempo de inferencia para cada imagen\n",
    "                print(f\"Tiempo de inferencia para {img_path}: {inference_time:.4f} segundos\")\n",
    "\n",
    "            # Calcular el tiempo de inferencia promedio\n",
    "            avg_inference_time = total_inference_time / num_images if num_images > 0 else 0\n",
    "\n",
    "            # Guardar el resultado\n",
    "            resultados_inferencia.append({\n",
    "                \"trial_number\": trial_number,\n",
    "                \"inference_time\": avg_inference_time\n",
    "            })\n",
    "\n",
    "    # Ordenar los resultados por tiempo de inferencia (menor a mayor)\n",
    "    resultados_inferencia.sort(key=lambda x: x[\"inference_time\"])\n",
    "\n",
    "    # Guardar los resultados en un archivo JSON\n",
    "    inference_times_path = os.path.join(ruta_resultados, \"inference_times.json\")\n",
    "    with open(inference_times_path, 'w') as f:\n",
    "        json.dump(resultados_inferencia, f, indent=4)\n",
    "\n",
    "    print(f\"Resultados guardados en {inference_times_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un .json con todas las métricas: Resultados.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los archivos JSON\n",
    "with open(evaluacion_path, 'r') as f:\n",
    "    evaluacion_data = json.load(f)\n",
    "\n",
    "with open(sorted_trials_path, 'r') as f:\n",
    "    sorted_trials_data = json.load(f)\n",
    "\n",
    "with open(all_metrics_path, 'r') as f:\n",
    "    all_metrics_data = json.load(f)\n",
    "\n",
    "with open(inference_times_path, 'r') as f:\n",
    "    inference_times_data = json.load(f)\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de sorted_trials\n",
    "sorted_trials_dict = {trial['trial_number']: trial['weighted_score'] for trial in sorted_trials_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de all_metrics\n",
    "all_metrics_dict = {trial['Trial']: trial for trial in all_metrics_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de inference_times\n",
    "inference_times_dict = {trial['trial_number']: trial['inference_time'] for trial in inference_times_data}\n",
    "\n",
    "# Obtener la información común del primer elemento de all_metrics_data\n",
    "if all_metrics_data:\n",
    "    common_info = {\n",
    "        \"Dataset\": all_metrics_data[0].get(\"Dataset\", \"\"),\n",
    "        \"Feedback\": all_metrics_data[0].get(\"Feedback\", \"\"),\n",
    "        \"Pre-Trained_Weights\": all_metrics_data[0].get(\"Pre-Trained_Weights\", \"\")\n",
    "    }\n",
    "else:\n",
    "    common_info = {\n",
    "        \"Dataset\": \"\",\n",
    "        \"Feedback\": \"\",\n",
    "        \"Pre-Trained_Weights\": \"\"\n",
    "    }\n",
    "\n",
    "# Crear el archivo de salida\n",
    "resultados = []\n",
    "\n",
    "# Ordenar los trial_numbers basados en sorted_trials_dict\n",
    "sorted_trial_numbers = sorted(sorted_trials_dict.keys())\n",
    "\n",
    "# Combinar la información\n",
    "for trial_number in sorted_trial_numbers:\n",
    "    # Buscar la información de evaluación y métricas\n",
    "    evaluation_info = next((item for item in evaluacion_data if item[\"trial_number\"] == trial_number), {})\n",
    "    metrics_info = all_metrics_dict.get(trial_number, {})\n",
    "    inference_time = inference_times_dict.get(trial_number, 0)  # Obtener el tiempo de inferencia\n",
    "\n",
    "    # Crear el objeto para el trial actual\n",
    "    trial_info = {\n",
    "        \"trial_number\": trial_number,\n",
    "        \"hyperparameters\": metrics_info.get(\"hyperparameters\", {}),\n",
    "        \"metrics\": metrics_info.get(\"metrics\", {}),\n",
    "        \"evaluation\": evaluation_info,\n",
    "        \"weighted_score\": sorted_trials_dict[trial_number],\n",
    "        \"inference_time\": inference_time  # Añadir el tiempo de inferencia\n",
    "    }\n",
    "\n",
    "    # Añadir la información común y la información del trial a la lista de resultados\n",
    "    resultados.append({**common_info, **trial_info})\n",
    "\n",
    "# Guardar el resultado en el archivo Resultados.json\n",
    "resultados_path = os.path.join(os.path.dirname(trial_dir), \"Resultados.json\")\n",
    "with open(resultados_path, 'w') as f:\n",
    "    json.dump(resultados, f, indent=2)\n",
    "\n",
    "print(f\"Archivo Resultados.json actualizado con tiempos de inferencia guardado en {resultados_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultados ordenados por las mejores metricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los archivos JSON\n",
    "with open(evaluacion_path, 'r') as f:\n",
    "    evaluacion_data = json.load(f)\n",
    "\n",
    "with open(sorted_trials_path, 'r') as f:\n",
    "    sorted_trials_data = json.load(f)\n",
    "\n",
    "with open(all_metrics_path, 'r') as f:\n",
    "    all_metrics_data = json.load(f)\n",
    "\n",
    "with open(inference_times_path, 'r') as f:\n",
    "    inference_times_data = json.load(f)\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de sorted_trials\n",
    "sorted_trials_dict = {trial['trial_number']: trial['weighted_score'] for trial in sorted_trials_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de all_metrics\n",
    "all_metrics_dict = {trial['Trial']: trial for trial in all_metrics_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de inference_times\n",
    "inference_times_dict = {trial['trial_number']: trial['inference_time'] for trial in inference_times_data}\n",
    "\n",
    "# Obtener la información común del primer elemento de all_metrics_data\n",
    "if all_metrics_data:\n",
    "    common_info = {\n",
    "        \"Dataset\": all_metrics_data[0].get(\"Dataset\", \"\"),\n",
    "        \"Feedback\": all_metrics_data[0].get(\"Feedback\", \"\"),\n",
    "        \"Pre-Trained_Weights\": all_metrics_data[0].get(\"Pre-Trained_Weights\", \"\")\n",
    "    }\n",
    "else:\n",
    "    common_info = {\n",
    "        \"Dataset\": \"\",\n",
    "        \"Feedback\": \"\",\n",
    "        \"Pre-Trained_Weights\": \"\"\n",
    "    }\n",
    "\n",
    "# Crear el archivo de salida\n",
    "resultados_ordenados = []\n",
    "\n",
    "# Extraer los trial_numbers en el orden exacto en el que aparecen en sorted_trials_data\n",
    "sorted_trial_numbers = [trial['trial_number'] for trial in sorted_trials_data]\n",
    "\n",
    "# Combinar la información\n",
    "for trial_number in sorted_trial_numbers:\n",
    "    # Buscar la información de evaluación y métricas\n",
    "    evaluation_info = next((item for item in evaluacion_data if item[\"trial_number\"] == trial_number), {})\n",
    "    metrics_info = all_metrics_dict.get(trial_number, {})\n",
    "    inference_time = inference_times_dict.get(trial_number, 0)  # Obtener el tiempo de inferencia\n",
    "\n",
    "    # Crear el objeto para el trial actual\n",
    "    trial_info = {\n",
    "        \"trial_number\": trial_number,\n",
    "        \"hyperparameters\": metrics_info.get(\"hyperparameters\", {}),\n",
    "        \"metrics\": metrics_info.get(\"metrics\", {}),\n",
    "        \"evaluation\": evaluation_info,\n",
    "        \"weighted_score\": sorted_trials_dict[trial_number],\n",
    "        \"inference_time\": inference_time  # Añadir el tiempo de inferencia\n",
    "    }\n",
    "\n",
    "    # Añadir la información común y la información del trial a la lista de resultados\n",
    "    resultados_ordenados.append({**common_info, **trial_info})\n",
    "\n",
    "# Guardar el resultado en el archivo con un nombre dinámico\n",
    "resultados_path = os.path.join(os.path.dirname(trial_dir), \"Resultados_ordenados_metricas.json\")\n",
    "with open(resultados_path, 'w') as f:\n",
    "    json.dump(resultados_ordenados, f, indent=2)\n",
    "\n",
    "print(f\"Archivo Resultados_ordenados_metricas.json guardado en {resultados_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultados ordenados por los mejores tiempos de inferencia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los archivos JSON\n",
    "with open(evaluacion_path, 'r') as f:\n",
    "    evaluacion_data = json.load(f)\n",
    "\n",
    "with open(sorted_trials_path, 'r') as f:\n",
    "    sorted_trials_data = json.load(f)\n",
    "\n",
    "with open(all_metrics_path, 'r') as f:\n",
    "    all_metrics_data = json.load(f)\n",
    "\n",
    "with open(inference_times_path, 'r') as f:\n",
    "    inference_times_data = json.load(f)\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de sorted_trials\n",
    "sorted_trials_dict = {trial['trial_number']: trial['weighted_score'] for trial in sorted_trials_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de all_metrics\n",
    "all_metrics_dict = {trial['Trial']: trial for trial in all_metrics_data}\n",
    "\n",
    "# Crear un diccionario para acceder rápidamente a la información de inference_times\n",
    "inference_times_dict = {trial['trial_number']: trial['inference_time'] for trial in inference_times_data}\n",
    "\n",
    "# Obtener la información común del primer elemento de all_metrics_data\n",
    "if all_metrics_data:\n",
    "    common_info = {\n",
    "        \"Dataset\": all_metrics_data[0].get(\"Dataset\", \"\"),\n",
    "        \"Feedback\": all_metrics_data[0].get(\"Feedback\", \"\"),\n",
    "        \"Pre-Trained_Weights\": all_metrics_data[0].get(\"Pre-Trained_Weights\", \"\")\n",
    "    }\n",
    "else:\n",
    "    common_info = {\n",
    "        \"Dataset\": \"\",\n",
    "        \"Feedback\": \"\",\n",
    "        \"Pre-Trained_Weights\": \"\"\n",
    "    }\n",
    "\n",
    "# Crear el archivo de salida\n",
    "resultados_ordenados = []\n",
    "\n",
    "# Extraer los trial_numbers en el orden exacto en el que aparecen en sorted_trials_data\n",
    "sorted_trial_numbers = [trial['trial_number'] for trial in sorted_trials_data]\n",
    "\n",
    "# Combinar la información\n",
    "for trial_number in sorted_trial_numbers:\n",
    "    # Buscar la información de evaluación y métricas\n",
    "    evaluation_info = next((item for item in evaluacion_data if item[\"trial_number\"] == trial_number), {})\n",
    "    metrics_info = all_metrics_dict.get(trial_number, {})\n",
    "    inference_time = inference_times_dict.get(trial_number, 0)  # Obtener el tiempo de inferencia\n",
    "\n",
    "    # Crear el objeto para el trial actual\n",
    "    trial_info = {\n",
    "        \"trial_number\": trial_number,\n",
    "        \"hyperparameters\": metrics_info.get(\"hyperparameters\", {}),\n",
    "        \"metrics\": metrics_info.get(\"metrics\", {}),\n",
    "        \"evaluation\": evaluation_info,\n",
    "        \"weighted_score\": sorted_trials_dict[trial_number],\n",
    "        \"inference_time\": inference_time  # Añadir el tiempo de inferencia\n",
    "    }\n",
    "\n",
    "    # Añadir la información común y la información del trial a la lista de resultados\n",
    "    resultados_ordenados.append({**common_info, **trial_info})\n",
    "\n",
    "# Ordenar los resultados por tiempo de inferencia (menor a mayor)\n",
    "resultados_ordenados.sort(key=lambda x: x[\"inference_time\"])\n",
    "\n",
    "# Guardar el resultado en el archivo con un nombre dinámico\n",
    "resultados_path = os.path.join(os.path.dirname(trial_dir), \"Resultados_ordenados_time.json\")\n",
    "with open(resultados_path, 'w') as f:\n",
    "    json.dump(resultados_ordenados, f, indent=2)\n",
    "\n",
    "print(f\"Archivo Resultados_ordenados_metricas.json guardado en {resultados_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV8_Detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
